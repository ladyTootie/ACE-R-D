{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJpTPK5IP+RpMoM9DfSTNO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ladyTootie/ACE-R-D/blob/main/MPL_Final_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import TensorFlow and Check the Version"
      ],
      "metadata": {
        "id": "KTJMjdF9AGpi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXhZKVmO94gH",
        "outputId": "7419edcc-4f0b-4f69-9650-16dedbace04c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Preprocess NSL-KDD dataset"
      ],
      "metadata": {
        "id": "TJ4ZO0Cg_8pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tensorflow.keras.models import Sequential #linear stack of layers\n",
        "from tensorflow.keras.layers import Dense #connected neural network layer\n",
        "\n",
        "#Define column names\n",
        "column_names = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
        "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
        "    'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n",
        "    'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds',\n",
        "    'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
        "    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
        "    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
        "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
        "    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
        "    'dst_host_srv_rerror_rate', 'traffic_type', 'difficulty_level' # Adjusted column names\n",
        "]\n",
        "\n",
        "#Load the dataset\n",
        "df = pd.read_csv('/content/KDDTrain+.txt', names=column_names)\n",
        "\n",
        "# Drop the difficulty_level column\n",
        "df = df.drop('difficulty_level', axis=1)\n",
        "\n",
        "#Check if dataset loaded correctly\n",
        "display(df.head())\n",
        "\n",
        "#Identify categorical and numerical features\n",
        "categorical_features = ['protocol_type', 'service', 'flag']\n",
        "numerical_features = [col for col in df.columns if col not in categorical_features + ['traffic_type']]\n",
        "\n",
        "# Convert numerical columns to numeric, coercing errors, and fill NaNs\n",
        "for col in numerical_features:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df[col] = df[col].fillna(df[col].mean()) # or .median()\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "X = df.drop('traffic_type', axis=1)\n",
        "y = df['traffic_type']\n",
        "\n",
        "# Preprocessing: One-hot encode categorical features and scale numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)])\n",
        "\n",
        "# Apply preprocessing\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "#print(\"Preprocessing complete. Data split into training and testing sets.\")\n",
        "#print(\"Shape of X_train:\", X_train.shape)\n",
        "#print(\"Shape of y_train:\", y_train.shape)\n",
        "#print(\"Shape of X_test:\", X_test.shape)\n",
        "#print(\"Shape of y_test:\", y_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "mcjjAiEqAN9U",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the MLP Model\n"
      ],
      "metadata": {
        "id": "xrNiFlVXIL2e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d384ae54"
      },
      "source": [
        "# Get the number of input features\n",
        "input_shape = X_train.shape[1]\n",
        "\n",
        "# Get the number of output classes\n",
        "output_shape = len(label_encoder.classes_)\n",
        "\n",
        "# Create the MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(input_shape,)), # Input layer and first hidden layer\n",
        "    Dense(64, activation='relu'), # Second hidden layer\n",
        "    Dense(output_shape, activation='softmax') # Output layer with softmax for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy', # Use sparse_categorical_crossentropy for integer labels\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "OQoeuaWVLmwP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a47a0460"
      },
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7c3e9c5"
      },
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Model evaluation complete.\")\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the conn.log"
      ],
      "metadata": {
        "id": "9xxfpHHeO0DI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31850cdd"
      },
      "source": [
        "\n",
        "# Load the new conn.log data using column names defined for NSL-KDD\n",
        "conn_df = pd.read_csv('/content/conn(2).log', names=column_names)\n",
        "\n",
        "\n",
        "# Check if the loaded data has the expected columns\n",
        "expected_features = [col for col in column_names if col not in ['difficulty_level', 'traffic_type']]\n",
        "if not all(col in conn_df.columns for col in expected_features):\n",
        "    print(\"Warning: Columns in conn.log do not match expected features.\")\n",
        "\n",
        "# Identify numerical and categorical features\n",
        "numerical_features_conn = [col for col in conn_df.columns if col not in categorical_features + ['traffic_type']]\n",
        "\n",
        "# Convert numerical columns to numeric, coercing errors, and fill NaNs\n",
        "for col in numerical_features_conn:\n",
        "    conn_df[col] = pd.to_numeric(conn_df[col], errors='coerce')\n",
        "    conn_df[col] = conn_df[col].fillna(df[col].mean()) # Use mean from original training df\n",
        "\n",
        "# Fill NaN values in categorical features with a placeholder\n",
        "for col in categorical_features:\n",
        "    conn_df[col] = conn_df[col].fillna('missing') # Fill with a string placeholder\n",
        "\n",
        "# Select the features using the correct lists\n",
        "X_conn = conn_df[numerical_features + categorical_features]\n",
        "\n",
        "\n",
        "# Apply the preprocessing transformation\n",
        "X_conn_processed = preprocessor.transform(X_conn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c22b0ba8"
      },
      "source": [
        "# Predict Anomalies and Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69844dc1"
      },
      "source": [
        "# Make predictions on the preprocessed conn.log data\n",
        "predictions = model.predict(X_conn_processed)\n",
        "\n",
        "# The predictions are probabilities for each class. We need to get the class with the highest probability.\n",
        "predicted_classes_encoded = tf.argmax(predictions, axis=1).numpy()\n",
        "\n",
        "# Decode the predicted classes back to their original labels\n",
        "predicted_classes = label_encoder.inverse_transform(predicted_classes_encoded)\n",
        "\n",
        "# Add the predicted traffic type to the conn_df DataFrame\n",
        "conn_df['predicted_traffic_type'] = predicted_classes\n",
        "\n",
        "# Define what constitutes an \"anomaly\"\n",
        "anomalies = conn_df[conn_df['predicted_traffic_type'] != 'normal']\n",
        "\n",
        "# Display the rows that are predicted as anomalies\n",
        "if not anomalies.empty:\n",
        "    print(\"Anomalies detected:\")\n",
        "    display(anomalies)\n",
        "else:\n",
        "    print(\"No anomalies detected\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}